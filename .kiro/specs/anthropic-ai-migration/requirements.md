# Requirements Document

## Introduction

This feature migrates the AI helper from the current OpenAI/Heroku-based implementation to a pure Anthropic-based solution using Claude Sonnet 4.5 as the default model. The migration includes implementing real-time text streaming for improved user experience and removing dependencies on Heroku Managed Inference and MCP functionality that requires Heroku infrastructure.

## Glossary

- **AI_Helper**: The MentorMind AI assistant component that provides study help to users
- **Anthropic_API**: The Claude AI inference API provided by Anthropic
- **Claude_Sonnet_4_5**: The default AI model (claude-sonnet-4-20250514) for the migrated system
- **Streaming_Response**: Real-time delivery of AI response tokens as they are generated
- **BYOK**: Bring Your Own Key - allows users to use their own API keys
- **SSE**: Server-Sent Events - protocol for streaming responses from server to client

## Requirements

### Requirement 1

**User Story:** As a user, I want to receive AI responses in real-time as they are generated, so that I can start reading the response immediately without waiting for the full completion.

#### Acceptance Criteria

1. WHEN a user sends a message to the AI helper THEN the AI_Helper SHALL stream response tokens to the client as they are generated by the Anthropic_API
2. WHEN streaming begins THEN the AI_Helper SHALL display partial response content incrementally in the message list
3. WHEN the stream completes THEN the AI_Helper SHALL finalize the message and update the UI to reflect completion state
4. IF the streaming connection is interrupted THEN the AI_Helper SHALL display an error message and allow the user to retry

### Requirement 2

**User Story:** As a developer, I want to use Anthropic's Claude Sonnet 4.5 as the default model, so that I can leverage its advanced reasoning capabilities for study assistance.

#### Acceptance Criteria

1. THE AI_Helper SHALL use Claude_Sonnet_4_5 (claude-sonnet-4-20250514) as the default inference model
2. WHEN the ANTHROPIC_API_KEY environment variable is configured THEN the AI_Helper SHALL authenticate requests using that key
3. WHEN making API requests THEN the AI_Helper SHALL use the Anthropic Messages API endpoint
4. THE AI_Helper SHALL support configurable max_tokens with a default of 4096 tokens

### Requirement 3

**User Story:** As a developer, I want to remove Heroku/MCP dependencies, so that the AI helper can function independently without external MCP infrastructure.

#### Acceptance Criteria

1. THE AI_Helper SHALL operate without requiring HEROKU_INFERENCE_URL configuration
2. THE AI_Helper SHALL operate without requiring MCP server connections
3. WHEN MCP tools are unavailable THEN the AI_Helper SHALL continue to function for basic chat interactions
4. THE AI_Helper SHALL remove references to Heroku Agents API endpoints

### Requirement 4

**User Story:** As a user, I want to see a loading indicator while the AI is generating a response, so that I know the system is working.

#### Acceptance Criteria

1. WHEN a message is sent THEN the AI_Helper SHALL display a loading indicator immediately
2. WHILE streaming is in progress THEN the AI_Helper SHALL show the partial response alongside a streaming indicator
3. WHEN streaming completes THEN the AI_Helper SHALL remove the loading indicator

### Requirement 5

**User Story:** As a developer, I want the system to handle errors gracefully, so that users receive helpful feedback when issues occur.

#### Acceptance Criteria

1. IF the Anthropic_API returns an error THEN the AI_Helper SHALL display a user-friendly error message
2. IF the API key is invalid or missing THEN the AI_Helper SHALL return a 401 status with an appropriate message
3. IF rate limiting occurs THEN the AI_Helper SHALL inform the user to try again later
4. WHEN an error occurs during streaming THEN the AI_Helper SHALL preserve any partial response already displayed

### Requirement 6

**User Story:** As a user with my own API key, I want to continue using BYOK functionality with multiple providers, so that I can use my personal API keys without coin charges.

#### Acceptance Criteria

1. WHEN a user has configured their own Anthropic API key THEN the AI_Helper SHALL use that key for Anthropic model requests
2. WHEN a user has configured their own OpenAI API key THEN the AI_Helper SHALL use that key for OpenAI-compatible model requests
3. WHEN a user has configured their own OpenRouter API key THEN the AI_Helper SHALL use that key for OpenRouter model requests
4. WHEN BYOK is used with any provider THEN the AI_Helper SHALL not charge coins for the query
5. WHEN BYOK is used THEN the AI_Helper SHALL indicate to the user which provider and key was used
6. THE AI_Helper SHALL support streaming responses for all BYOK providers (Anthropic, OpenAI, OpenRouter)
